3.1 线性回归基本概念:
    线性回归算法是解决监督式学习中回归问题的重要算法.

    3.1.1 线性模型:
        hwb(x) = <w, x > + b

    线性回归算法:
        输入: m条训练数据S
        输出: 线性模型hwb(x) = <w*, x > +b*

        其中w*, b*是优化问题:
            min: MSE(x, y)
        的最优解.

    3.1.2 均方误差:
        MSE(x, y) = sum(hwb(x)-y) ^ 2 / m (m为样本数)
        该值表征线性模型拟合样本的程度, 越小, 表示该线性模型能更好的拟合样本的分布趋势.

    3.1.3 似然函数
        随机变量Y的联合分布函数(或联合分布密度函数)为似然函数.
            L(y | w) = multiply(Pw(Y=yi)

        最大似然原则:
            若w*是使L(y | w)最大化的一组参数, 那么判定Y的概率分布使pw*.

    在线性回归中, 假设样本, 标签都服从正态分布.
    如果hwb(x)是标签的期望, 那么:
        标签服从正态分布N(hwb(x), sigma)
    不妨设sigma=1, 那么:
        该问题的似然函数 L(y | w, b) = multiply(1 / squar(2 * Pi) * exp{- (hwb (xi) - yi) ^ 2})
    其与均方误差的关系有:
        argmax:L(y | w, b) = argmin:MSE(x, y)
        证明:
            对似然函数取对数有:
                log(L(y | w, b)) = m * log(1 / squar(2 * Pi) - sum(hub(x) - yi) ^ 2 (m为样本数)
            故 max:L(y | w, b)  等价于 min:MSE(x, y)
    
    线性回归算法:
        为了简化记号,通常令:w~ = (b,w) ,x~ = (1,x),
            那么hwb(x~) = <W~, x~>
        简化后的算法表述如下:
            输入: m条训练数据S
            输出:hwb(x) = <w*,x>
            w*为优化问题min: MSE(x, y)的最优解.
    


