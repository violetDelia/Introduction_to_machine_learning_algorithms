3.2: 线性回归优化算法:
    正规方程算法也称最小二乘算法.是线性回归最基础的算法.
    
    3.2.1 线性回归的均方误差
        F(w) = MSE(x, y)
        是关于w的可微凸函数.
        证明:
            因为hwb(x) - y是关于w的凸函数(线性函数是凸函数),所以(hwb(x)-y) ^ 2也是凸函数.
            所以sum((hwb(x)-y) ^ 2)/m = F(w) 是凸函数.

    由此,我们可以写出线性回归算法的目标函数:
        min:F(w) = ||X @ w-y|| ^ 2
        其中: X 为特征矩阵
              y 为标签向量
    
    3.2.2 线性回归目标函数的最优解:
        因为F(w)是凸函数,所以
            F'(w*) = 0 = 2 * X.T @ X @ w - 2 * X.T @ y 
            当 (X.T @ X)可逆时有最优解:
                w* = inv(X.T @ X) @ X.T @ y
    
    因为均方误差时回归问题的目标函数,因此均方误差本身就可以作为算法效果的度量.但是均方误差不能直观的
    判断建立模型的拟合效果,为了能够直观的度量模型的效果,引入决定系数:
    3.2.3 决定系数
        设 y~ 是训练数据标签的平均值,定义:
            R^2 =1 - (sum(hwb(x)-y) ^ 2) / (sum(y~ - y) ^ 2)
        为模型h的决定系数.
        R^2的值越接近1,拟合效果越好.

    3.2.4 特征标准化
        在一个实际问题中,特征组各分量往往处于不同的量级之中,这会使得在计算均方误差的时候量级大的的分量
        主导模型的训练,忽视量级较小的特征分量.因此需要标准化特征组,使各分量处在同一个量级之中.

        给定m条训练数据S,对于每一个分量计算其均值和方差:
            u = average(xi)
            sigma = D(xi)
        令 xi = (xi-u)/squar(sigma)

        经过标准化后,特征组均处在同一个量级,有利于算法的计算.(在标准化之后在加入简化分量(1))

    因为有最优解的条件是(X.T @ X)可逆,当特征组之间不相互独立或者特征数n>m(训练样本数时),该条件是不成立的.
    并且该算法要对矩阵求逆,求逆算法的复杂度是O（n^3),因此在特征组多的情况下算法效率不好.

    
        


        
    

