3.2: 线性回归优化算法:
    正规方程算法也称最小二乘算法.是线性回归最基础的算法.
    
    3.2.1 线性回归的均方误差
        F(w) = MSE(x, y)
        是关于w的可微凸函数.
        证明:
            因为hwb(x) - y是关于w的凸函数(线性函数是凸函数),所以(hwb(x)-y) ^ 2也是凸函数.
            所以sum((hwb(x)-y) ^ 2)/m = F(w) 是凸函数.

    由此,我们可以写出线性回归算法的目标函数:
        min:F(w) = ||X @ w-y|| ^ 2
        其中: X 为特征矩阵
              y 为标签向量
    
    3.2.2 线性回归目标函数的最优解:
        因为F(w)是凸函数,所以
            F'(w*) = 0 = 2 * X.T @ X @ w - 2 * X.T @ y 
            当 (X.T @ X)可逆时有最优解:
                w* = inv(X.T @ X) @ X.T @ y
    
    因为均方误差时回归问题的目标函数,因此均方误差本身就可以作为算法效果的度量.但是均方误差不能直观的
    判断建立模型的拟合效果,为了能够直观的度量模型的效果,引入决定系数:
    3.2.3 决定系数
        设 y~ 是训练数据标签的平均值,定义:
            R^2 =1 - (sum(hwb(x)-y) ^ 2) / (sum(y~ - y) ^ 2)
        为模型h的决定系数.
        R^2的值越接近1,拟合效果越好.
    
        


        
    

