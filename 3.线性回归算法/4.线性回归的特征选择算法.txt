3.5 线性回归的特征选择算法

    监督学习是拟合多个特征和标签的关系，但在实际应用中，并不是所有特征都与标签有关联。
    这样不仅增加了计算量，还影响了标签的预测结果。
    并且在训练数据规模较小的时候，无关的特征还会提高过度拟合的概率。
    Lasso回归的正则化有剔除无关特征的功能（特征的权重直接变为0），这也是该方法能降低过度拟合概率的根本原因。

    3.5.1 逐步回归
        逐步回归采用贪心算法策略来选择特征。根据均方误差来判断特征与标签是否有关联。
        如果一个特征引入能显著的减小模型的均方误差，那么认为它与标签有关联的。

        向前逐步回归是最简单的逐步回归算法：
            前置定义：
                1.给定集合A{1,...,n},以及向量x{x1,...xn}。xA为x中下标属于A的分量构成的向量。
                2.设X为m*n的矩阵，给定集合A{1,...,n},XA是由X中下标属于A的列构成的矩阵。
            
            算法描述：
                已选特征集合A和备选集合C;
                for (int i = 0; i <n; ++i)
                    先求的已选特征集A的均方误差MESA;
                    （可以再次检验有没有需要去除的特征值）
                    for(int j = 0; j <C.size(); ++j)  （不一定只能加一列特征，也可以加一组特征）
                        在求得该特征值加入A后的最优均方误差MESAJ;
                    从中寻找到最优的特征列C[j]*;
                    if（显著减小了均方误差）
                        将该列从C移动进A;
                    else
                        break;
                return A

        逐步回归采用贪心算法策略来寻找特征值，所以可能找出的不是最优解。

    3.5.2 分段回归

        3.5.3 相关系数：
            corr(u,v) = <u,v>/(||u||*||v||)
            该值u，v向量夹角的余弦值。
            因此相关系数的绝对值越大，u，v的相关性越强。

        分段回归算法描述：
        func(X,y,max_steps,learning_rate)
            w = （0 , ... , 0),step = 0
            while (step < max_steps) 
                #计算出预测误差
                distances = y - Xw
                #找出与预测误差相关度最高的特征j*
                j* = argmax|corr(Xj,distances)|
                #沿着最优特征Xj迭代，如果正相关，前进，反之后退。
                w = w + learning_rate * sign(Xj,distances)*Xj
                t = t + 1
            return w
        
        分段回归是通过局部微调来接近最优特征值选取的。但是有两个缺点：
            1：需要大量的迭代搜索，运行效率低。
            2：过于保守，使其降低过度拟合的效果减弱。

    分段回归和逐步回归各有千秋，需要通过实验来寻找更合适的特征选取算法。



        

        

            
                    




