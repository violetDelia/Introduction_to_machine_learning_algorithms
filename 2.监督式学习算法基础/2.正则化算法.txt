2.4 正则化算法:
    引言:
        在计算最小经验损失的过程中,需要合理选择模型假设来避免过度拟合.
        尽管训练数据规模足够大时从理论上可以避免过度拟合.
        但实际中训练数据往往时有限制的.
        因此需要一些策略防止过度拟合.

    前置定义:
        在机器学习中,模型往往可以用有限个是参数表示:
            一个n元d次多项式模型h可以表示为:
                h(x) = w1*x1^a1 + w2*x2^a2 +.....
                s.t. a1+a2+a3... >= 0
                    a1+a2+a3... <= d
        在以后的表述中hw表示该模型,其中w = {w1,w2,w3,...}为该模型的参数.

    正则化与范数:
        在机器学习中,我们经常采用w的范数来量化模型的复杂度.l1范数和l2范数是最常用
        的范数定义.
        在机器学习中,普遍认为w的范数越小,模型越简单.
        如果两个模型在训练数据上的经验损失接近,应该选用参数范数较小的那一个.
        经验损失最小化算法中的正则化方法体现了这一思想.

    正则化方法的经验损失最小化算法:
        模型假设H = {hw,参数w定义域为实数空间}
        输入:m条训练数据S
            计算优化问题的最优解w*
                min:exp_loss(hw)+r|w|
        输出:hw*

        其中 r|w|被称为 正则化因子
                r被称为 正则化系数
        对于每一个非0的参数wi,都会对目标函数带来r|wi|的惩罚,这会引导算法在exp_loss()
        在取值接近的情况下选出惩罚项最小的那个模型.

    注: 目前没有一个一般的公式可以确定正则化系数r的取值,因此算法设计需要通过实验获取

    l1正则化的缺点:
        目标函数不可微(0点处),因此引入l2正则化.

    l1与l2的正则化的特点:
        l1引导算法将参数w的分量直接下降成0,从而降低模型的次数.
        l2引导算法将参数w的分量均匀的减小,增加模型的平滑度.

    l1和l2的应用:
        l1多用于特征选择.
        l2多用于增加模型的稳定性.

    注: 正则化经验损失最小化算法实际应用是一类启发式算法,没有理论性的保证.





